{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b70d7d56-b9c1-4f36-bacb-cacaa0828e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# V 0.1 \n",
    "# basic functionalities. read and export Train List, Occupancy, and Booking Payment Detailed\n",
    "# \n",
    "# V 0.2 \n",
    "# handle all the information of each kind of report together. \n",
    "#\n",
    "# v 0.3\n",
    "# add general logging\n",
    "# delete all the information in the same query\n",
    "# added importation by chunks\n",
    "# added information of the process of each day\n",
    "#\n",
    "# v 0.4\n",
    "# correct a bug to insert properly insertion entry to audit table\n",
    "# add an error log and alert window to alert for errors\n",
    "# categorize all the message printed by their kind\n",
    "# \n",
    "# v 0.41\n",
    "# fixed bug missing some logging level information during exportation\n",
    "# added message when deleting the day\n",
    "# fixed minor bug regarding the number of entries inserted that are shown in screen\n",
    "#\n",
    "# v 0.5\n",
    "# extend the number of rows to be checked for the header to 50\n",
    "# delete incorrect rows at the end\n",
    "# fixed minor bug with the exporting\n",
    "# read all the sheets of an excel file\n",
    "# improve the detection of the wrong lines at the end. saves them in file\n",
    "# compress result as zip\n",
    "# unify timestamps\n",
    "# arrange output in folders\n",
    "# save duplicates in file\n",
    "# \n",
    "# v 0.51\n",
    "# fixed the train key of occupancy\n",
    "#\n",
    "# v 0.52\n",
    "# fixed the export error messages to be included in the corresponding log\n",
    "# save the export data file before exportation to keep it in case of error during exportation\n",
    "# fix the wrong rows not being saved\n",
    "#\n",
    "# v 0.53\n",
    "# fixed a bug regarding the operation date not taking the minimum value\n",
    "#\n",
    "# v 0.54\n",
    "# fixing the error management in some functions\n",
    "# limiting some fields to not dump database\n",
    "#\n",
    "# v 0.55\n",
    "# fixing errors with the telephone truncation\n",
    "# \n",
    "# v 0.6\n",
    "# fixing management of errors\n",
    "# add error line\n",
    "# add error management when audit insertion\n",
    "# modify pool recycle to avoid recycle sessions too soon\n",
    "#\n",
    "# v 0.61\n",
    "# adjusting the parameters of alchemy engine\n",
    "# missing traceback exportation\n",
    "#\n",
    "# v 0.7\n",
    "# using the copy function to export\n",
    "#\n",
    "# v 0.71\n",
    "# fix a bug with the train hour\n",
    "#\n",
    "# v 0.72\n",
    "# fix a bug when notifying missing trains\n",
    "# added some error management in train list reader\n",
    "#\n",
    "# v 0.73\n",
    "# remove some train lists columns from being mandatory\n",
    "# add counter for wrong entries\n",
    "#\n",
    "# v 0.74\n",
    "# save the duplicated entries\n",
    "# accept any empty value in train list except the ticket number\n",
    "#\n",
    "# v 0.75\n",
    "# fix a bug when saving the duplicates\n",
    "# \n",
    "# v 0.8\n",
    "# update the constraint for train list\n",
    "# deactivate the constraints in database before the updating of train list\n",
    "# add more information to the read process\n",
    "#\n",
    "# v 0.81\n",
    "# limit the constraint removal when the number of rows exceeed certain threshold\n",
    "#\n",
    "# v 0.82\n",
    "# fixed the version in the exporter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "23eb465f-e7a6-4825-ac11-7b330b4ccf08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connection to the database\n",
    "# import psycopg2\n",
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "# Create an engine instance\n",
    "alchemyEngine = create_engine(\connection_string, \n",
    "                              pool_recycle=-1,  # Recycle connections after 1 hour\n",
    "                              pool_pre_ping=True,  # Test connections before using\n",
    "                              pool_size = 1,\n",
    "                              max_overflow=10\n",
    "                             );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "90c6a0f0-6e67-4c55-be5b-69907cdd60cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "39a1be9b-b12b-4563-93d1-245c0a6902bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import \n",
    "import pandas as pd\n",
    "from openpyxl import load_workbook\n",
    "from numpy import sort as np_sort\n",
    "import datetime\n",
    "import os\n",
    "import warnings\n",
    "import sys\n",
    "import logging\n",
    "import tkinter as tk\n",
    "from tkinter import messagebox\n",
    "import shutil\n",
    "import traceback\n",
    "from io import StringIO\n",
    "#import concurrent.futures\n",
    "from threading import Thread\n",
    "from pathos.pp import ParallelPool\n",
    "from multiprocessing import cpu_count\n",
    "from math import ceil\n",
    "\n",
    "\n",
    "\n",
    "# tkinter\n",
    "root = tk.Tk()\n",
    "root.withdraw()  # Hide the root window\n",
    "\n",
    "# STATES\n",
    "NO_REPORT = 0\n",
    "TRAIN_LIST_REPORT = 1\n",
    "OCCUPANCY_REPORT = 2\n",
    "BOOKING_PAYMENT_REPORT = 3\n",
    "\n",
    "# PARAMETERS\n",
    "rows_threshold_constraint_removal = 400000\n",
    "\n",
    "# ERRORS FOUND\n",
    "errors_found = False\n",
    "\n",
    "# timestamp for all the records\n",
    "current_time = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "# folders\n",
    "log_folder = \"log\"\n",
    "export_folder = \"export\"\n",
    "data_folder = \"data\"\n",
    "\n",
    "# tables\n",
    "#train_list_table = 'train_list'\n",
    "#occupancy_table = 'occupancy_list_hist'\n",
    "#bpd_table = 'booking_payment_detailed'\n",
    "\n",
    "train_list_table = 'train_list'\n",
    "occupancy_table = 'occupancy_list_hist'\n",
    "bpd_table = 'booking_payment_detailed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b46c9a85-608e-448a-bde3-3033480cb3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_error_line(e):    \n",
    "    tb = traceback.format_exc()  # Get the full traceback as a string\n",
    "    line_number = traceback.extract_tb(e.__traceback__)[-1].lineno  # Get the line number of the error\n",
    "    return line_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c9c648c4-f9d6-412a-a237-467688f3c451",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prt_info(string, kind=logging.INFO, nl=True):\n",
    "    global errors_found\n",
    "    current_time = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    \n",
    "    # Get the current time\n",
    "    info = f\"[{current_time}] {string}\"\n",
    "\n",
    "    info = '\\r'+info\n",
    "\n",
    "    # log the message in the different loggers\n",
    "    log.log(kind, info)\n",
    "    \n",
    "    # record there is error or warning to set the alert window at the end\n",
    "    if kind >= logging.WARNING:\n",
    "        error_log.log(kind, info)\n",
    "        errors_found = True\n",
    "\n",
    "    # Print the information\n",
    "    print(info, end='' if not nl else '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ff81a607-e938-4a64-96ac-eb017940aa5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up logging\n",
    "log_name = 'exportation_' + current_time + '.log'\n",
    "\n",
    "# create the directory if not exist\n",
    "if not os.path.exists(log_folder):\n",
    "    os.makedirs(log_folder)\n",
    "\n",
    "log = logging.getLogger(\"log_general\")\n",
    "log.setLevel(logging.INFO) \n",
    "general_handler = logging.FileHandler(f\"{log_folder}/{log_name}\", mode='w')\n",
    "general_handler.setLevel(logging.INFO)\n",
    "log.addHandler(general_handler)\n",
    "\n",
    "error_log = logging.getLogger(\"log_error\")\n",
    "error_handler = logging.FileHandler(f\"{log_folder}/error_{log_name}\", mode='w')\n",
    "error_handler.setLevel(logging.WARNING)\n",
    "error_log.addHandler(error_handler)\n",
    "\n",
    "#debug_log = logging.getLogger(\"log_debug\")\n",
    "#debug_handler = logging.FileHandler(\"debug_\" + log_name, mode='w')\n",
    "#debug_handler.setLevel(logging.DEBUG)\n",
    "# debug_log.addHandler(debug_handler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "605a995c-d1f1-4539-87d6-94396745528a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# version control\n",
    "version = 0.82\n",
    "is_final_version = False\n",
    "max_control = 0\n",
    "version_checked = True\n",
    "\n",
    "\n",
    "# check if the current version is the last one\n",
    "try:\n",
    "    query = \"SELECT version FROM \\\"AFC\\\".exporter_version_control\"\n",
    "    all_versions = pd.read_sql_query(query, alchemyEngine)\n",
    "    max_version = all_versions['version'].max()\n",
    "except Exception as e:\n",
    "    prt_info(\"There was a problem fetching the last version of the program. Skipping version check\", logging.ERROR)\n",
    "    version_checked = False\n",
    "\n",
    "try:\n",
    "    if version > max_version and is_final_version:\n",
    "        # add the new version\n",
    "        with alchemyEngine.connect() as conn:\n",
    "            query = text(f\"insert into \\\"AFC\\\".exporter_version_control(date, version) values (\\'{datetime.datetime.now()}\\',\\'{version}\\')\")\n",
    "            conn.execute(query)\n",
    "            conn.commit()\n",
    "        \n",
    "    elif version < max_version and version_checked:\n",
    "        # this program is out of date, terminating execution\n",
    "        prt_info(\"Current exporter is out of date. Please, use the last version to export data.\", logging.ERROR)\n",
    "        sys.exit()\n",
    "\n",
    "    #else:\n",
    "        #current is the last version. Do nothing\n",
    "\n",
    "except Exception as e:\n",
    "    prt_info(\"The version could not be checked in the database\", logging.ERROR)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "3a3f3f76-9e01-4666-91c6-45ac0eb035be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that detects which kind of report is the excel file\n",
    "def get_report_name(excel_file_path, sheet=0):\n",
    "\n",
    "    train_list_header = pd.DataFrame([\n",
    "        'Departure Date',\n",
    "        'Train Number',\n",
    "        'OD',\n",
    "        'Origin Station',\n",
    "        'Destination Station',\n",
    "        'Coach Number',\n",
    "        'Seat Number',\n",
    "        'Class',\n",
    "        'Booking Code',\n",
    "        'Ticket Number',\n",
    "        'Tariff',\n",
    "        'Status',\n",
    "        'Payment Mode',\n",
    "        'Media Type',\n",
    "        'Sales Channel',\n",
    "        'Base Price',\n",
    "        'VAT Base Price',\n",
    "        'Management Fee',\n",
    "        'VAT Management Fee',\n",
    "        'Payment Fee',\n",
    "        'VAT Payment Fee',\n",
    "        'Operation Amount',\n",
    "        'Penalty Tariff',\n",
    "        'Amount Not Refunded',\n",
    "        'Compensation Type',\n",
    "        'Compensation Reason',\n",
    "        'Compensation Status',\n",
    "        'Nationality',\n",
    "        'Gender',\n",
    "        'Name',\n",
    "        'Surname',\n",
    "        'Document',\n",
    "        'Prefix',\n",
    "        'Telephone',\n",
    "        'Profile',\n",
    "        'Special Needs',\t\n",
    "        'Validation Time',\n",
    "        'Group',\n",
    "        'Checked On Board',\n",
    "        'Last Operation Channel',\n",
    "        'Last Operation Equipment Code'\n",
    "        ])\n",
    "    \n",
    "    occupancy_header = pd.DataFrame([\n",
    "        'Date',\n",
    "        'OD',\n",
    "        'Origin Station',\n",
    "        'Destination Station',\n",
    "        'Train ID',\n",
    "        'Train Number',\n",
    "        'Class',\n",
    "        'Total Seats (Quota + Carer + PRM)',\n",
    "        'Quota Configuration',\n",
    "        'Total Locks (Quota + Carer + PRM)',\n",
    "        'For Sale',\n",
    "        'Reserved Usual Seats',\n",
    "        'Reserved PRM Seats',\n",
    "        'Reserved Carer Seats',\t\n",
    "        'Ticket Reserved (Usual + Carer + PRM)',\n",
    "        'Reserved & Lock Usual Seats',\n",
    "        'Reserved & Lock PRM Seats',\n",
    "        'Reserved & Lock Carer Seats',\t\n",
    "        'Total Available',\n",
    "        'Validating',\n",
    "        'No Show',\n",
    "        'UnBooked',\t\n",
    "        'Passengers Inc. Infants',\n",
    "        'Checked On Board'\n",
    "    ])\n",
    "    \n",
    "    bpd_header = pd.DataFrame([\n",
    "       'Booking Code',\n",
    "       'Ticket Number',\t\n",
    "       'Operation Date',\t\n",
    "       'Base Price',\n",
    "       'VAT Base Price',\n",
    "       'Management Fee',\n",
    "       'VAT Management Fee',\n",
    "       'Payment Fee',\n",
    "       'VAT Payment Fee',\n",
    "       'Operation Amount',\t\n",
    "       'Penalty Tariff',\t\n",
    "       'Compensation Type',\t\n",
    "       'Compensation Reason',\t\n",
    "       'Compensation Status',\n",
    "       'Card Number',\n",
    "       'Authorization Code',\n",
    "       'Order ID',\n",
    "       'Transaction ID',\n",
    "       'Status Payment Card',\n",
    "       'Card Brand',\n",
    "       'Bill Number',\n",
    "       'Bill Status',\n",
    "       'Train Number',\t\n",
    "       'Departure Date',\t\n",
    "       'Arrival Date',\n",
    "       'OD',\n",
    "       'Origin Station',\n",
    "       'Destination Station',\n",
    "       'Class',\n",
    "       'Tariff',\t\n",
    "       'Reserved Number of Seats',\n",
    "       'Status',\n",
    "       'Card Serial Number',\n",
    "       'Card User Name',\n",
    "       'Sales Station',\n",
    "       'Sales Channel',\n",
    "       'Sales Equipment Code',\n",
    "       'Payment Mode',\n",
    "       'Coach Number',\t\n",
    "       'Seat Number',\n",
    "       'Nationality',\n",
    "       'Name',\n",
    "       'Surname',\n",
    "       'Gender',\n",
    "       'Document Type',\n",
    "       'Document',\n",
    "       'Prefix',\n",
    "       'Telephone',\n",
    "       'Email',\n",
    "       'Profile',\t\n",
    "       'Validation Time',\n",
    "       'Checked On Board',\t\n",
    "       'Detail Type',\n",
    "       'Tipology',\n",
    "       'Last Operation Channel',\n",
    "       'Last Operation Equipment Code'\n",
    "    \n",
    "    ])\n",
    "    \n",
    "    try:\n",
    "        # read the header of the file\n",
    "        file_header = pd.read_excel(excel_file_path, sheet_name=sheet, nrows=50, header=None)\n",
    "    except Exception as e:\n",
    "        prt_info(f\"There is a problem reading the file: {e}\", kind=logging.ERROR)\n",
    "        return 0, NO_REPORT\n",
    "    \n",
    "    # go through the read part of the file\n",
    "    for index, row in enumerate(file_header.values):\n",
    "        # clean\n",
    "        row = row[pd.notnull(row)]\n",
    "        row = pd.DataFrame(row)\n",
    "        \n",
    "        # comparision\n",
    "        if(row.equals(train_list_header)): return index + 1, TRAIN_LIST_REPORT\n",
    "        elif(row.equals(occupancy_header)): return index + 1, OCCUPANCY_REPORT\n",
    "        elif(row.equals(bpd_header)): return index + 1, BOOKING_PAYMENT_REPORT\n",
    "            \n",
    "    # no report found\n",
    "    return 0, NO_REPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b2d595f3-48e0-4767-a4e0-a9d4de96d5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallel_pandas_read_excel(file_name, header, sheet_name, skiprows, dtype):\n",
    "    #calculate the paramters\n",
    "    min_rows_per_process = 3000\n",
    "    first_row = skiprows + 1\n",
    "    header_row = first_row + header\n",
    "    max_row = load_workbook(file_name)[sheet_name].max_row\n",
    "    total_num_rows = max_row - skiprows\n",
    "    num_processes = min(cpu_count() - 1, ceil(total_num_rows / min_rows_per_process))\n",
    "    tier_num_rows = ceil(total_num_rows / num_processes)\n",
    "\n",
    "    if num_processes == 1:\n",
    "        # no parallelism\n",
    "        return pandas_read_excel([file_name, sheet_name, dtype, skiprows+1, max_row, header_row])\n",
    "    else:\n",
    "        #init\n",
    "        \n",
    "        next_row = first_row\n",
    "        dfs = []\n",
    "        tiers = []\n",
    "        while next_row <= max_row:\n",
    "            end_row = min(next_row + tier_num_rows, max_row)\n",
    "            tiers.append([file_name, sheet_name, dtype, next_row, end_row, header_row])\n",
    "            next_row = end_row + 1 \n",
    "    \n",
    "        #read\n",
    "        with ParallelPool(nodes=num_processes) as pool:\n",
    "            return pd.concat(pool.map(pandas_read_excel, tiers))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "7e2056fd-8ed0-494d-8ced-704fb2e83ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pandas_read_excel(args):\n",
    "    import sys\n",
    "    \n",
    "    if 'pandas' not in sys.modules:\n",
    "        import pandas as pd  # Import pandas if not already imported\n",
    "    else:\n",
    "        pd = sys.modules['pandas']  # Use the already imported pandas module\n",
    "\n",
    "    if 'warnings' not in sys.modules:\n",
    "        import warnings\n",
    "    else:\n",
    "        warnings = sys.modules['warnings']  \n",
    "\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    try:\n",
    "        \n",
    "        #unpack\n",
    "        file_name, sheet_name, dtype, from_row, to_row, header_row = args\n",
    "        if (to_row < from_row):\n",
    "            return pd.DataFrame() #empty dataframe\n",
    "        else:\n",
    "            #set the header\n",
    "            if header_row < from_row or header_row > to_row:\n",
    "                header = None\n",
    "                has_header = 0\n",
    "            else:\n",
    "                header = header_row - from_row\n",
    "                has_header = 1\n",
    "                \n",
    "            #read the file\n",
    "            return pd.read_excel(file_name, header=header, sheet_name=sheet_name, skiprows=(from_row-1), nrows=(to_row - from_row + 1 - has_header), dtype=dtype)\n",
    "    except Exception as e:\n",
    "        #prt_info(f\"Error with the reading function\")\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "34b18b48-847c-4889-9e93-a112717e12b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_train_list(file_name, #alchemyEngine, \n",
    "                    sheet=0):\n",
    "\n",
    "    global current_time\n",
    "    \n",
    "    # train_list datatype\n",
    "    train_list_datatype = {\n",
    "    'Departure Date': str,\n",
    "    'Train Number': str,\n",
    "    'OD': str,\n",
    "    'Origin Station': str,\n",
    "    'Destination Station': str,\n",
    "    'Coach Number': str,\n",
    "    'Seat Number': str,\n",
    "    'Class': str,\n",
    "    'Booking Code': str,\n",
    "    'Ticket Number': str,\n",
    "    'Tariff': str,\n",
    "    'Status': str,\n",
    "    'Payment Mode': str,\n",
    "    'Media Type': str,\n",
    "    'Sales Channel': str,\n",
    "    'Base Price': float,\n",
    "    'VAT Base Price': float,\n",
    "    'Management Fee': float,\n",
    "    'VAT Management Fee': float,\n",
    "    'Payment Fee': float,\n",
    "    'VAT Payment Fee': float,\n",
    "    'Operation Amount':\tfloat,\n",
    "    'Penalty Tariff': float,\n",
    "    'Amount Not Refunded': float,\n",
    "    'Compensation Type': str,\n",
    "    'Compensation Reason': str,\n",
    "    'Compensation Status': str,\n",
    "    'Nationality': str,\n",
    "    'Gender': str,\n",
    "    'Name': str,\n",
    "    'Surname': str,\n",
    "    'Document': str,\n",
    "    'Prefix': str,\n",
    "    'Telephone': str,\n",
    "    'Profile': str,\n",
    "    'Special Needs': str,\t\n",
    "    'Validation Time': str,\n",
    "    'Group': str,\n",
    "    'Checked On Board': str,\n",
    "    'Last Operation Channel': str,\n",
    "    'Last Operation Equipment Code': str\n",
    "    }\n",
    "    \n",
    "    # get the first line of the report\n",
    "    first_row, name_report = get_report_name(file_name, sheet)\n",
    "    \n",
    "    if (name_report != TRAIN_LIST_REPORT):\n",
    "        raise Exception(f\"Wrong function invoked for sheet '{sheet}' from '{file_name}'\")\n",
    "        \n",
    "    #read the file\n",
    "    prt_info(f\"Reading sheet '{sheet}' from '{file_name}'...\")\n",
    "    try:\n",
    "        # open file\n",
    "        df_file = pd.read_excel(file_name, header=0, sheet_name=sheet, skiprows=(first_row-1), dtype=str)\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Error opening the file: {e}\")\n",
    "        \n",
    "    #prt_info(f\"Formatting...\")\n",
    "    try:\n",
    "        #delete empty columns\n",
    "        df_file = df_file.loc[:, ~df_file.columns.str.contains('^Unnamed')]\n",
    "        \n",
    "        # format date columns\n",
    "        date_cols = []\n",
    "        for col in date_cols:\n",
    "            df_file[col] = pd.to_datetime(df_file[col], errors='coerce', format='%Y-%m-%d')\n",
    "        \n",
    "        # format datetime columns\n",
    "        datetime_cols = ['Departure Date','Validation Time']\n",
    "        for col in datetime_cols:\n",
    "            df_file[col] = pd.to_datetime(df_file[col], errors='coerce', format='%Y-%m-%d %H:%M:%S')\n",
    "        \n",
    "        # format numeric columns\n",
    "        num_cols = [\n",
    "            'VAT Base Price',\n",
    "            'Management Fee',\n",
    "            'VAT Management Fee',\n",
    "            'Payment Fee',\n",
    "            'VAT Payment Fee',\n",
    "            'Operation Amount',\n",
    "            'Penalty Tariff',\n",
    "            'Amount Not Refunded']\n",
    "        \n",
    "        for col in num_cols:\n",
    "            df_file[col] = pd.to_numeric(df_file[col], errors='coerce')\n",
    "\n",
    "        \n",
    "        # turn nullable columns to empty space\n",
    "        not_null_cols = [    \n",
    "            'Departure Date',\n",
    "            'Train Number',\n",
    "            'OD',\n",
    "            'Origin Station',\n",
    "            'Destination Station',\n",
    "            'Class',\n",
    "            'Booking Code',\n",
    "            'Ticket Number',\n",
    "            'Tariff',\n",
    "            'Status',\n",
    "            'Base Price',\n",
    "            'VAT Base Price',\n",
    "            'Management Fee',\n",
    "            'VAT Management Fee',\n",
    "            'Payment Fee',\n",
    "            'VAT Payment Fee',\n",
    "            'Operation Amount'\n",
    "        ]\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Error formatting columns: {e}\")\n",
    "        \n",
    "    #prt_info(f\"Cleaning...\")\n",
    "    try:\n",
    "        # check wrong lines. delete all nan or nat and save them in a separate file\n",
    "        df_nan = df_file[df_file[not_null_cols].isna().any(axis=1)]\n",
    "        df_file.dropna(inplace=True, how='any', subset=not_null_cols)\n",
    "        if df_nan is not None:\n",
    "            prt_info(f\"{df_nan.shape[0]} registers from Train List have been removed because they were incorrect entries.\")\n",
    "            if not os.path.exists(export_folder):\n",
    "                os.makedirs(export_folder)\n",
    "            df_nan.to_csv(f\"{export_folder}/Train List error rows {current_time}.csv.zip\")\n",
    "        \n",
    "        #check if there are still data entries\n",
    "        if df_file.shape[0] == 0:\n",
    "            raise Exception(f\"Empty dataset after cleaning.\")\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Error cleaning data: {e}\")\n",
    "\n",
    "    # get the train departure\n",
    "    try:\n",
    "        train_hours = pd.read_sql_table('train_departure_times', alchemyEngine, schema='AFC', parse_dates={'departure_time':'%H:%M:%S'})\n",
    "\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Error fetching the departure times from database: {e}\")\n",
    "\n",
    "    #prt_info(f\"Calculating extra fields...\")\n",
    "    try:    \n",
    "        train_hours.columns = ['Train Number', 'Train Hour']\n",
    "        df_file = pd.merge(df_file, train_hours, on=\"Train Number\", how=\"left\")\n",
    "    \n",
    "        #check if there is missing hours for the train numbers of this file\n",
    "        if(df_file['Train Hour'].isnull().sum() > 0):\n",
    "            trains_missing = df_file['Train Number'][df_file['Train Hour'].isnull()].unique()\n",
    "            raise Exception(f\"There are missing departing hours in the database. Please, check the following trains: {', '.join(trains_missing)}\")\n",
    "            \n",
    "        # create extra columns\n",
    "        df_file['Train Hour'] = df_file['Train Hour'].dt.strftime('%H:%M')\n",
    "        df_file['Departure_Date_Short'] = df_file['Departure Date'].dt.strftime('%Y-%m-%d')\n",
    "        df_file['Train-OD Short'] = df_file['Train Number'] + \" - \" + df_file['OD']\n",
    "        df_file['CORRIDOR'] = df_file['Train Number'].str[:2]\n",
    "        df_file['WEEK_DAY'] = df_file['Departure Date'].dt.strftime('%a')\n",
    "        df_file['WEEK_NUM'] = df_file['Departure Date'].dt.isocalendar().week\n",
    "        df_file['train_key'] = df_file['Departure_Date_Short'] + \" - \" + df_file['Train-OD Short']\n",
    "        \n",
    "        # calculate the departing time of the train\n",
    "        df_file['train_departure_date_time'] = pd.to_datetime(df_file['Departure_Date_Short'].astype(str) + \" \" + df_file['Train Hour'].astype(str))\n",
    "        train_date_adjustment = df_file['train_departure_date_time'].dt.time > df_file['Departure Date'].dt.time\n",
    "        df_file['train_departure_date_time'] = df_file['train_departure_date_time'] - pd.to_timedelta(train_date_adjustment.astype(int), unit=\"D\")\n",
    "        df_file['train_departure_date_short'] = df_file['Departure Date'].dt.date - pd.to_timedelta(train_date_adjustment.astype(int), unit=\"D\")\n",
    "        \n",
    "        # calculate the services date (reduce one day if it is an early train before maintenance window)\n",
    "        service_date_adjustment = df_file['train_departure_date_time'].dt.time <= datetime.time(5, 0)\n",
    "        df_file['Service_Date'] = df_file['train_departure_date_short'] - pd.to_timedelta(service_date_adjustment.astype(int), unit=\"D\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Error calculating the new columns: {e}\")\n",
    "        \n",
    "    try:    \n",
    "        # get the date time of the operation\n",
    "        ticket_numbers = \", \".join(f\"'{ticket}'\" for ticket in df_file['Ticket Number'].unique())\n",
    "        query = f\"\"\"\n",
    "        SELECT ticket_number AS \\\"Ticket Number\\\", min(operation_date_time) as operation_date_time\n",
    "        FROM \\\"AFC\\\".{bpd_table}\n",
    "        WHERE ticket_number IN ({ticket_numbers})\n",
    "        GROUP BY ticket_number\n",
    "        \"\"\"\n",
    "        \n",
    "        df_operation_date_times = pd.read_sql_query(query, alchemyEngine)\n",
    "        df_file = pd.merge(df_file, df_operation_date_times, on=\"Ticket Number\", how=\"left\")\n",
    "        df_file['operation_date'] = pd.to_datetime(df_file['operation_date_time'], errors='coerce', format='%Y-%m-%d %H:%M:%S').dt.strftime(\"%Y-%m-%d\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Error adding the train time: {e}\")\n",
    "\n",
    "    try:\n",
    "        # transform columns date and datetime to text\n",
    "        for col in datetime_cols:\n",
    "            df_file[col] = df_file[col].dt.strftime('%Y-%m-%d %H:%M')\n",
    "            \n",
    "        for col in date_cols:\n",
    "            df_file[col] = df_file[col].dt.strftime('%Y-%m-%d')\n",
    "\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Error reformatting datetime columns: {e}\")\n",
    "\n",
    "    # truncate the columns that exceed the ddbb size\n",
    "    import re\n",
    "\n",
    "    try:\n",
    "        # Define a function to clean each row\n",
    "        def clean_phone(row):\n",
    "            # Remove any matching prefix\n",
    "            cleaned = re.sub(rf'^{re.escape(row[\"Prefix\"])}', '', row['Telephone'])\n",
    "            # Remove dashes\n",
    "            cleaned = cleaned.replace('-', '')\n",
    "            return cleaned\n",
    "        \n",
    "        # Apply the function row-wise\n",
    "        df_file['Telephone'] = df_file.apply(clean_phone, axis=1)\n",
    "        df_file['Telephone'] = df_file['Telephone'].str.slice(0,14)\n",
    "    \n",
    "        # clean the empty cells\n",
    "        df.replace(\"\",None, inplace=True)\n",
    "        df.replace(\" \",None, inplace=True)\n",
    "\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Error while truncating columns: {e}\")\n",
    "\n",
    "    try:\n",
    "        # set the headers according to database\n",
    "        df_file.columns = [\n",
    "        'departure_date', \n",
    "        'train_number', \n",
    "        'od', \n",
    "        'origin_station', \n",
    "        'destination_station',\n",
    "        'coach_number', \n",
    "        'seat_number', \n",
    "        'class', \n",
    "        'booking_code', \n",
    "        'ticket_number', \n",
    "        'tariff', \n",
    "        'status', \n",
    "        'payment_mode', \n",
    "        'media_type', \n",
    "        'sales_channel', \n",
    "        'base_price', \n",
    "        'vat_base_price',\n",
    "        'management_fee', \n",
    "        'vat_management_fee', \n",
    "        'payment_fee', \n",
    "        'vat_payment_fee', \n",
    "        'operation_amount', \n",
    "        'penalty_tariff', \n",
    "        'amount_not_refunded', \n",
    "        'compensation_type', \n",
    "        'compensation_reason', \n",
    "        'compensation_status', \n",
    "        'nationality', \n",
    "        'gender', \n",
    "        'name', \n",
    "        'surname', \n",
    "        'document', \n",
    "        'prefix', \n",
    "        'telephone', \n",
    "        'profile', \n",
    "        'special_needs', \n",
    "        'validating_time', \n",
    "        'groupyn', \n",
    "        'checked_on_board', \n",
    "        'last_operation_channel', \n",
    "        'last_operation_equipment_code', \n",
    "        'train_hour', \n",
    "        'departure_date_short', \n",
    "        'train_od_short', \n",
    "        'stretch', \n",
    "        'week_day', \n",
    "        'week_num', \n",
    "        'train_key', \n",
    "        'train_departure_date_time', \n",
    "        'train_departure_date_short', \n",
    "        'service_train_departure_date_short', \n",
    "        'operation_date_time', \n",
    "        'operation_date']\n",
    "\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Error while naming the headers: {e}\")\n",
    "\n",
    "    return df_file\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ab08a1d6-63fb-4744-b4f6-ed06ff87f82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_booking_payment(file_name, sheet=0):\n",
    "\n",
    "    booking_payment_datatype = {\n",
    "        'Booking Code':str,\n",
    "       'Ticket Number':str,\t\n",
    "       'Operation Date':str,\t\n",
    "       'Base Price':float,\n",
    "       'VAT Base Price':float,\n",
    "       'Management Fee':float,\n",
    "       'VAT Management Fee':float,\n",
    "       'Payment Fee':float,\n",
    "       'VAT Payment Fee':float,\n",
    "       'Operation Amount':float,\t\n",
    "       'Penalty Tariff':float,\n",
    "        #amount no refunded\n",
    "       'Compensation Type':str,\t\n",
    "       'Compensation Reason':str,\t\n",
    "       'Compensation Status':str,\n",
    "       'Card Number':str,\n",
    "       'Authorization Code':str,\n",
    "       'Order ID':str,\n",
    "       'Transaction ID':str,\n",
    "       'Status Payment Card':str,\n",
    "       'Card Brand':str,\n",
    "       'Bill Number':str,\n",
    "       'Bill Status':str,\n",
    "       'Train Number':str,\t\n",
    "       'Departure Date':str,\t\n",
    "       'Arrival Date':str,\n",
    "       'OD':str,\n",
    "       'Origin Station':str,\n",
    "       'Destination Station':str,\n",
    "       'Class':str,\n",
    "       'Tariff':str,\t\n",
    "       'Reserved Number of Seats':str,\n",
    "       'Status':str,\n",
    "       'Card Serial Number':str,\n",
    "       'Card User Name':str,\n",
    "       'Sales Station':str,\n",
    "       'Sales Channel':str,\n",
    "       'Sales Equipment Code':str,\n",
    "       'Payment Mode':str,\n",
    "       'Coach Number':str,\t\n",
    "       'Seat Number':str,\n",
    "       'Nationality':str,\n",
    "       'Name':str,\n",
    "       'Surname':str,\n",
    "       'Gender':str,\n",
    "       'Document Type':str,\n",
    "       'Document':str,\n",
    "       'Prefix':str,\n",
    "       'Telephone':str,\n",
    "       'Email':str,\n",
    "       'Profile':str,\t\n",
    "       'Validation Time':str,\n",
    "       'Checked On Board':str,\t\n",
    "       'Detail Type':str,\n",
    "       'Tipology':str,\n",
    "       'Last Operation Channel':str,\n",
    "       'Last Operation Equipment Code':str\n",
    "    }\n",
    "\n",
    "    # get the first line of the report\n",
    "    first_row, name_report = get_report_name(file_name, sheet)\n",
    "\n",
    "    if (name_report != BOOKING_PAYMENT_REPORT):\n",
    "        raise Exception(f\"Wrong function invoked for sheet '{sheet}' from '{file_name}'\")\n",
    "    \n",
    "    # read\n",
    "    try:\n",
    "        df_file = pd.read_excel(file_name, header=0, sheet_name=sheet, skiprows=(first_row-1), dtype=str)\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Error opening the file: {e}\")\n",
    "    \n",
    "    #delete empty columns\n",
    "    df_file = df_file.loc[:, ~df_file.columns.str.contains('^Unnamed')]\n",
    "    \n",
    "    # format date columns\n",
    "    date_cols = []\n",
    "    for col in date_cols:\n",
    "        df_file[col] = pd.to_datetime(df_file[col], errors='coerce', format='%Y-%m-%d')\n",
    "    \n",
    "    # format datetime columns\n",
    "    datetime_cols = ['Operation Date','Departure Date','Arrival Date']\n",
    "    for col in datetime_cols:\n",
    "        df_file[col] = pd.to_datetime(df_file[col], errors='coerce', format='%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "    # format numeric columns\n",
    "    num_cols = [\n",
    "        'Base Price',\n",
    "        'VAT Base Price',\n",
    "        'Management Fee',\n",
    "        'VAT Management Fee',\n",
    "        'Payment Fee',\n",
    "        'VAT Payment Fee',\n",
    "        'Operation Amount',\n",
    "        'Penalty Tariff']\n",
    "    \n",
    "    for col in num_cols:\n",
    "        df_file[col] = pd.to_numeric(df_file[col], errors='coerce')\n",
    "    \n",
    "    # turn nullable columns to empty space\n",
    "    nullable_cols = [\n",
    "       'Compensation Type',\t\n",
    "       'Compensation Reason',\t\n",
    "       'Compensation Status',\n",
    "       'Card Number',\n",
    "       'Authorization Code',\n",
    "       'Order ID',\n",
    "       'Transaction ID',\n",
    "       'Status Payment Card',\n",
    "       'Card Brand',\n",
    "       'Bill Number',\n",
    "       'Bill Status',\n",
    "       'Reserved Number of Seats',\n",
    "       'Card Serial Number',\n",
    "       'Card User Name',\n",
    "       'Sales Station',\n",
    "       'Sales Equipment Code',\n",
    "       'Coach Number',\t\n",
    "       'Seat Number',\n",
    "       'Nationality',\n",
    "       'Name',\n",
    "       'Surname',\n",
    "       'Gender',\n",
    "       'Document Type',\n",
    "       'Document',\n",
    "       'Prefix',\n",
    "       'Telephone',\n",
    "       'Email',\n",
    "       'Profile',\t\n",
    "       'Validation Time',\n",
    "       'Checked On Board',\t\n",
    "       'Detail Type',\n",
    "       'Tipology',\n",
    "       'Last Operation Channel',\n",
    "       'Last Operation Equipment Code']\n",
    "    \n",
    "    # check wrong lines. delete all nan or nat and save them in a separate file\n",
    "    df_nan = df_file[df_file[df_file.columns.difference(nullable_cols)].isna().any(axis=1)]\n",
    "    df_file.dropna(inplace=True, how='any', subset=df_file.columns.difference(nullable_cols))\n",
    "    if not df_nan.empty:\n",
    "        prt_info(f\"{df_nan.shape[0]} registers from Booking Payment Detailed have been removed because they were incorrect entries.\")\n",
    "        if not os.path.exists(export_folder):\n",
    "            os.makedirs(export_folder)\n",
    "        df_nan.to_csv(f\"{export_folder}/Booking Payment Detailed error rows {current_time}.csv.zip\")\n",
    "    \n",
    "    #check if there are still data entries\n",
    "    if df_file.shape[0] == 0:\n",
    "        raise Exception(f\"Empty dataset after cleaning.\")   \n",
    "\n",
    "    # transform columns date and datetime to text\n",
    "    for col in datetime_cols:\n",
    "        df_file[col] = df_file[col].dt.strftime('%Y-%m-%d %H:%M')\n",
    "        \n",
    "    for col in date_cols:\n",
    "        df_file[col] = df_file[col].dt.strftime('%Y-%m-%d')\n",
    "\n",
    "    # clean the empty cells\n",
    "    df.replace(\"\",None, inplace=True)\n",
    "    df.replace(\" \",None, inplace=True)\n",
    "    \n",
    "    # set column names\n",
    "    df_file.columns = [\n",
    "        'booking_code', \n",
    "        'ticket_number',\n",
    "        'operation_date_time', \n",
    "        'base_price', \n",
    "        'base_price_vat', \n",
    "        'management_fee', \n",
    "        'management_fee_vat', \n",
    "        'payment_fee', \n",
    "        'payment_fee_vat', \n",
    "        'operation_amount', \n",
    "        'penalty_tariff', \n",
    "        #'amount_not_refunded', \n",
    "        'compensation_type', \n",
    "        'compensation_reason', \n",
    "        'compensation_status', \n",
    "        'card_number', \n",
    "        'authorization_code', \n",
    "        'order_id', \n",
    "        'transaction_id', \n",
    "        'status_payment_card', \n",
    "        'card_brand', \n",
    "        'bill_number', \n",
    "        'bill_status', \n",
    "        'train_number', \n",
    "        'departure_date_time', \n",
    "        'arrival_date_time', \n",
    "        'od', \n",
    "        'origin_station', \n",
    "        'destination_station', \n",
    "        'class', \n",
    "        'tariff', \n",
    "        'reserved_number_of_seats', \n",
    "        'status', \n",
    "        'card_serial_number', \n",
    "        'card_user_name', \n",
    "        'sales_station', \n",
    "        'sales_channel', \n",
    "        'equipment_code', \n",
    "        'payment_mode', \n",
    "        'coach_number', \n",
    "        'seat_number', \n",
    "        'country_code', \n",
    "        'name', \n",
    "        'surname', \n",
    "        'gender', \n",
    "        'document_type', \n",
    "        'document', \n",
    "        'prefix', \n",
    "        'telephone', \n",
    "        'email', \n",
    "        'profile', \n",
    "        'validating_time', \n",
    "        'checked_on_board', \n",
    "        'detail_type', \n",
    "        'tipology', \n",
    "        #'compensated', \n",
    "        #'include_fare_revenue', \n",
    "        'last_operation_channel', \n",
    "        'last_operation_equipment_code'\n",
    "    ]\n",
    "    # return\n",
    "    return df_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "da4684c4-cb9e-4a89-9ffc-9843f34cb42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_occupancy(file_name, sheet=0):    \n",
    "    \n",
    "    # define the datatype\n",
    "    occupancy_datatype = {\n",
    "        'Date':str,\n",
    "        'OD':str,\n",
    "        'Origin Station':str,\n",
    "        'Destination Station':str,\n",
    "        'Train ID':str,\n",
    "        'Train Number':str,\n",
    "        'Class':str,\n",
    "        'Total Seats (Quota + Carer + PRM)':str,\n",
    "        'Quota Configuration':str,\n",
    "        'Total Locks (Quota + Carer + PRM)':str,\n",
    "        'For Sale':str,\n",
    "        'Reserved Usual Seats':str,\n",
    "        'Reserved PRM Seats':str,\n",
    "        'Reserved Carer Seats':str,\t\n",
    "        'Ticket Reserved (Usual + Carer + PRM)':str,\n",
    "        'Reserved & Lock Usual Seats':str,\n",
    "        'Reserved & Lock PRM Seats':str,\n",
    "        'Reserved & Lock Carer Seats':str,\t\n",
    "        'Total Available':str,\n",
    "        'Validating':str,\n",
    "        'No Show':str,\n",
    "        'UnBooked':str,\t\n",
    "        'Passengers Inc. Infants':str,\n",
    "        'Checked On Board':str\n",
    "    }\n",
    "\n",
    "    # get the first line of the report\n",
    "    first_row, name_report = get_report_name(file_name, sheet)\n",
    "\n",
    "    if (name_report != OCCUPANCY_REPORT):\n",
    "        raise Exception(f\"Wrong function invoked for sheet '{sheet}' from '{file_name}'\")\n",
    "        \n",
    "    try:\n",
    "        df_file = pd.read_excel(file_name, header=0, skiprows=(first_row-1), sheet_name=sheet, dtype=occupancy_datatype)#, parse_dates=['Date'], date_format={'Date':'%Y-%m-%d'})\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Error opening the file: {e}\")\n",
    "    \n",
    "    #delete empty columns\n",
    "    df_file = df_file.loc[:, ~df_file.columns.str.contains('^Unnamed')]\n",
    "                \n",
    "    # format date columns\n",
    "    date_cols = []\n",
    "    for col in date_cols:\n",
    "        df_file[col] = pd.to_datetime(df_file[col], errors='coerce', format='%Y-%m-%d')\n",
    "    \n",
    "    # format datetime columns\n",
    "    datetime_cols = ['Date']\n",
    "    for col in datetime_cols:\n",
    "        df_file[col] = pd.to_datetime(df_file[col], errors='coerce', format='%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "    # format numeric columns\n",
    "    num_cols = []\n",
    "    \n",
    "    for col in num_cols:\n",
    "        df_file[col] = pd.to_numeric(df_file[col], errors='coerce')\n",
    "    \n",
    "    # turn nullable columns to empty space\n",
    "    nullable_cols = [\n",
    "        'Origin Station',\n",
    "        'Destination Station',\n",
    "        'Train ID',\n",
    "        'Total Seats (Quota + Carer + PRM)',\n",
    "        'Total Locks (Quota + Carer + PRM)',\n",
    "        'For Sale',\n",
    "        'Reserved Usual Seats',\n",
    "        'Reserved PRM Seats',\n",
    "        'Reserved Carer Seats',\t\n",
    "        'Reserved & Lock Usual Seats',\n",
    "        'Reserved & Lock PRM Seats',\n",
    "        'Reserved & Lock Carer Seats',\t\n",
    "        'Total Available',\n",
    "        'Validating',\n",
    "        'No Show',\n",
    "        'UnBooked',\t\n",
    "        'Passengers Inc. Infants',\n",
    "        'Checked On Board']\n",
    "    \n",
    "    # check wrong lines. delete all nan or nat and save them in a separate file\n",
    "    df_nan = df_file[df_file[df_file.columns.difference(nullable_cols)].isna().any(axis=1)]\n",
    "    df_file.dropna(inplace=True, how='any', subset=df_file.columns.difference(nullable_cols))\n",
    "    if not df_nan.empty:\n",
    "        prt_info(f\"{df_nan.shape[0]} registers from Occupancy have been removed because they were incorrect entries.\")\n",
    "        if not os.path.exists(export_folder):\n",
    "            os.makedirs(export_folder)\n",
    "        df_nan.to_csv(f\"{export_folder}/Occupancy error rows {current_time}.csv.zip\")\n",
    "    \n",
    "    #check if there are still data entries\n",
    "    if df_file.shape[0] == 0:\n",
    "        raise Exception(f\"Empty dataset after cleaning.\")\n",
    "    \n",
    "    # transform columns date and datetime to text\n",
    "    datetime_cols = []\n",
    "    for col in datetime_cols:\n",
    "        df_file[col] = df_file[col].dt.strftime('%Y-%m-%d %H:%M')\n",
    "\n",
    "    date_cols = ['Date'] #date is read as datetime but need to be parsed as date to make sure that duplicates are corretly found\n",
    "    for col in date_cols:\n",
    "        df_file[col] = df_file[col].dt.strftime('%Y-%m-%d')\n",
    "        \n",
    "    # create the extra columns\n",
    "    df_file['Data_Date'] = datetime.date.today()\n",
    "    df_file['train_key'] = pd.to_datetime(df_file['Date']).dt.strftime('%Y-%m-%d') + \" - \" + df_file['Train Number'] + \" - \" + df_file['OD']\n",
    "\n",
    "    # clean the empty cells\n",
    "    df.replace(\"\",None, inplace=True)\n",
    "    df.replace(\" \",None, inplace=True)\n",
    "    \n",
    "    # rename the columns\n",
    "    df_file.columns = [\n",
    "        'date', \n",
    "        'od', \n",
    "        'origin_station', \n",
    "        'destination_station', \n",
    "        'train_id', \n",
    "        'train_number', \n",
    "        'class', \n",
    "        'total_seats', \n",
    "        'quota_configuration', \n",
    "        'total_locks', \n",
    "        'for_sale', \n",
    "        'reserved_usual_seats', \n",
    "        'reserved_prm_seats', \n",
    "        'reserved_carer_seats', \n",
    "        'ticket_reserved', \n",
    "        'reserved_lock_usual_seats', \n",
    "        'reserved_lock_prm_seats', \n",
    "        'reserved_lock_carer_seats', \n",
    "        'total_available', \n",
    "        'validating', \n",
    "        'no_show', \n",
    "        'unbooked', \n",
    "        'passengers_inc_infant', \n",
    "        'checked_on_board', \n",
    "        'data_date', \n",
    "        'train_key'\n",
    "    ]\n",
    "      \n",
    "    # return\n",
    "    return df_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "3819a751-b9a7-482d-91be-a9e949d90040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get each pair of dates (beginning, end) of the streak days in the submitted list of dates\n",
    "def get_date_pairs(df, column):\n",
    "    try:\n",
    "        date_pairs = []\n",
    "        date_col = np_sort(pd.to_datetime(df[column]).dt.date.unique())\n",
    "        date_begin = date_col.min()\n",
    "        date_end = date_col.min()\n",
    "        day_count = (date_col.max() - date_col.min()).days + 1\n",
    "        \n",
    "        # if there is a single date, return that\n",
    "        if(len(date_col) == 1):\n",
    "            return [[date_col.min(), date_col.max()]]\n",
    "        \n",
    "        # iterate through the dates\n",
    "        for d in date_col:\n",
    "            #skip the first date\n",
    "            if(d == date_col.min()): continue\n",
    "        \n",
    "            # check if it is continous\n",
    "            if((d - date_end).days == 1):\n",
    "                date_end = d\n",
    "            else:\n",
    "                date_pairs.append([date_begin.strftime('%Y-%m-%d'), date_end.strftime('%Y-%m-%d')])\n",
    "                date_begin = d\n",
    "                date_end = d\n",
    "        #at the end insert the last element\n",
    "        if(date_begin is not None):\n",
    "            date_pairs.append([date_begin.strftime('%Y-%m-%d'), date_end.strftime('%Y-%m-%d')])\n",
    "            \n",
    "    \n",
    "        return date_pairs\n",
    "    \n",
    "    except Exception as e: \n",
    "        raise Exception(f\"There was an error while reading the dates of the report: {e}\", logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "52f419de-7c5c-4ace-9685-ed0391bd58f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_train_list(df_file, db_schema):\n",
    "    \n",
    "    # Extract unique dates from the DataFrame\n",
    "    unique_dates = df_file['departure_date_short'].unique()\n",
    "    date_conditions = ', '.join([f\"'{date}'\" for date in unique_dates])\n",
    "    \n",
    "    # variables of the ddbb\n",
    "    table_name = train_list_table\n",
    "    columns = df_file.columns.tolist()\n",
    "    \n",
    "    # Write the DataFrame to the PostgreSQL table\n",
    "    with alchemyEngine.connect().execution_options(autocommit=True) as conn:\n",
    "        \n",
    "        # get the dates of the record\n",
    "        date_pairs = get_date_pairs(df_file, 'departure_date_short')\n",
    "\n",
    "        # notice a warning if there are missing dates in the middle of the data\n",
    "        if(len(date_pairs) >1):\n",
    "            prt_info(\"The dates on the report Train List are not consecutive. Make sure all the files of the day has been submitted\", kind=logging.WARNING)\n",
    "                \n",
    "        # delete the previous records\n",
    "        for date_from, date_to in date_pairs:\n",
    "            try:\n",
    "                delete_query = text(f\"DELETE FROM \\\"{db_schema}\\\".{table_name} WHERE departure_date_short between \\'{date_from}\\' and \\'{date_to}\\'\")\n",
    "                conn.execute(delete_query)\n",
    "                conn.commit()\n",
    "                prt_info(f\"Previous data from {date_from} to {date_to} deleted successfully.\")\n",
    "            except Exception as e:\n",
    "                conn.rollback()\n",
    "                prt_info(f\"An error occurred while deleting the previous Train List data from {date_from} to {date_to} {e}\", kind=logging.ERROR)\n",
    "\n",
    "        # insert the data day by day\n",
    "        for date, group in df_file.groupby('departure_date_short'):\n",
    "            # Insert the data for the current date\n",
    "            \n",
    "            try:\n",
    "                #chunk_size = 500\n",
    "                #for chunk in range(0, len(group), chunk_size):\n",
    "                #    df_chunk = group.iloc[chunk:chunk + chunk_size]\n",
    "                #    df_chunk.to_sql(table_name, conn, schema=db_schema, if_exists='append', index=False)\n",
    "                #    conn.commit()\n",
    "                #    if ((chunk+chunk_size)<len(group)): prt_info(f\"Data for {date}: {chunk+chunk_size} entries inserted.\", kind=logging.DEBUG, nl=False)\n",
    "\n",
    "                # Get the raw DBAPI connection\n",
    "                raw_conn = conn.connection\n",
    "                cursor = raw_conn.cursor()\n",
    "\n",
    "                # create the buffer\n",
    "                buffer = StringIO()\n",
    "                group.to_csv(buffer, index=False, header=False) \n",
    "                buffer.seek(0) \n",
    "\n",
    "                # Use the COPY command\n",
    "                cursor.copy_expert(f\"\"\"\n",
    "                    COPY \\\"{db_schema}\\\".{table_name}({', '.join(columns)}) FROM STDIN WITH CSV DELIMITER ',';\n",
    "                \"\"\", buffer)                \n",
    "                \n",
    "                prt_info(f\"Data for {date} inserted successfully ({group.shape[0]} inserted).\")\n",
    "            except Exception as e:\n",
    "                conn.rollback()\n",
    "                raise Exception(f\"An error occurred while inserting Train List data for {date}: {e}\")\n",
    "            \n",
    "            #register the audit table\n",
    "            try:\n",
    "                audit_query = text(f\"INSERT INTO \\\"AFC\\\".audit(timestamp, \\\"table\\\", operation, period, \\\"user\\\") VALUES (\\'{datetime.datetime.now().strftime('%Y-%m-%d %H:%M')}\\', \\'{table_name}\\', \\'insert\\', \\'{date}\\', \\'{os.getlogin()}\\')\")\n",
    "                conn.execute(audit_query)\n",
    "                conn.commit()\n",
    "            except Exception as e:\n",
    "                conn.rollback()\n",
    "                raise Exception(f\"An error occurred while writing the audit information for Train List: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "3a140b83-233b-4805-ac86-488fc1bc1c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_booking_payment(df_file, db_schema):\n",
    "    \n",
    "    # Extract unique dates from the DataFrame\n",
    "    dates = pd.to_datetime(df_file['operation_date_time'], format=\"%Y-%m-%d %H:%M\").dt.date\n",
    "    unique_dates = dates.unique()\n",
    "    \n",
    "    # variables of the ddbb\n",
    "    table_name = bpd_table\n",
    "\n",
    "\n",
    "    # Get the column names from the DataFrame\n",
    "    columns = df_file.columns.tolist()\n",
    "    \n",
    "    # Write the DataFrame to the PostgreSQL table\n",
    "    with alchemyEngine.connect().execution_options(autocommit=True) as conn:\n",
    "        conn.autocommit = True\n",
    "            \n",
    "        for date in unique_dates:\n",
    "            group = df_file[dates == date]\n",
    "            \n",
    "            # Delete existing records for the current date\n",
    "            try:\n",
    "                delete_query = text(f\"DELETE FROM \\\"{db_schema}\\\".{table_name} WHERE to_char(operation_date_time, 'yyyy-mm-dd') = \\'{date}\\'\")\n",
    "                conn.execute(delete_query)\n",
    "                conn.commit()\n",
    "                prt_info(f\"Previous data for {date} deleted successfully.\")\n",
    "            except Exception as e:\n",
    "                conn.rollback()\n",
    "                prt_info(f\"An error occurred while deleting the previous Booking Payment Detailed data for date {date}: {e}\", kind=logging.ERROR)\n",
    "            \n",
    "            # Insert the data for the current date\n",
    "            \n",
    "            try:\n",
    "                # chunk_size = 500\n",
    "                #for chunk in range(0, len(group), chunk_size):\n",
    "                #    df_chunk = group.iloc[chunk:chunk + chunk_size]\n",
    "                #    df_chunk.to_sql(table_name, conn, schema=db_schema, if_exists='append', index=False)\n",
    "                #    conn.commit()\n",
    "                #    if ((chunk+chunk_size)<len(group)): prt_info(f\"Data for {date}: {chunk+chunk_size} entries inserted.\", kind=logging.DEBUG, nl=False)\n",
    "                \n",
    "                # Get the raw DBAPI connection\n",
    "                raw_conn = conn.connection\n",
    "                cursor = raw_conn.cursor()\n",
    "\n",
    "                # create the buffer\n",
    "                buffer = StringIO()\n",
    "                group.to_csv(buffer, index=False, header=False) \n",
    "                buffer.seek(0) \n",
    "\n",
    "                # Use the COPY command\n",
    "                cursor.copy_expert(f\"\"\"\n",
    "                    COPY \\\"{db_schema}\\\".{table_name}({', '.join(columns)}) FROM STDIN WITH CSV DELIMITER ',';\n",
    "                \"\"\", buffer)\n",
    "                \n",
    "                prt_info(f\"Data for {date} inserted successfully ({group.shape[0]} inserted).\")\n",
    "            except Exception as e:\n",
    "                conn.rollback()\n",
    "                raise Exception(f\"An error occurred while inserting Booking Payment Detailed data for {date}: {e}\")\n",
    "    \n",
    "            #register the audit table\n",
    "            try:\n",
    "                audit_query = text(f\"INSERT INTO \\\"AFC\\\".audit(timestamp, \\\"table\\\", operation, period, \\\"user\\\") VALUES (\\'{datetime.datetime.now().strftime('%Y-%m-%d %H:%M')}\\', \\'{table_name}\\', \\'insert\\', \\'{date}\\', \\'{os.getlogin()}\\')\")\n",
    "                conn.execute(audit_query)\n",
    "                conn.commit()\n",
    "            except Exception as e:\n",
    "                conn.rollback()\n",
    "                raise Exception(f\"An error occurred while writing the audit information for Booking Payment Detailed: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "a690c858-dae7-4111-b639-7f4290fcc444",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_occupancy(df_file, db_schema):\n",
    "  \n",
    "    # Extract unique dates from the DataFrame\n",
    "    dates = pd.to_datetime(df_file['date']).dt.strftime('%Y-%m-%d')\n",
    "    unique_dates = dates.sort_values().unique()\n",
    "    today = datetime.date.today().strftime('%Y-%m-%d')\n",
    "    \n",
    "    # variables of the ddbb\n",
    "    table_name = occupancy_table\n",
    "    columns = df_file.columns.tolist()\n",
    "    \n",
    "    # Write the DataFrame to the PostgreSQL table\n",
    "    with alchemyEngine.connect().execution_options(autocommit=True) as conn:\n",
    "        # get the dates of the record\n",
    "        date_pairs = get_date_pairs(df_file, 'date')\n",
    "\n",
    "        # notice a warning if there are missing dates in the middle of the data\n",
    "        if(len(date_pairs) >1):\n",
    "            prt_info(\"The dates on the report Occupancy are not consecutive. Make sure all the files of the day has been submitted\", kind=logging.WARNING)\n",
    "        \n",
    "        # delete the previous records\n",
    "        for date_from, date_to in date_pairs:\n",
    "            try:\n",
    "                delete_query = text(f\"DELETE FROM \\\"{db_schema}\\\".{table_name} WHERE to_char(date, 'yyyy-mm-dd') between \\'{date_from}\\' and \\'{date_to}\\'and data_date = \\'{today}\\'\")\n",
    "                conn.execute(delete_query)\n",
    "                conn.commit()\n",
    "                prt_info(f\"Previous data from {date_from} to {date_to} deleted successfully.\")\n",
    "            except Exception as e:\n",
    "                conn.rollback()\n",
    "                prt_info(f\"An error occurred while deleting the previous Occupancy data from {date_from} to {date_to}: {e}\", kind=logging.ERROR)\n",
    "    \n",
    "        for date in unique_dates:\n",
    "            group = df_file[dates == date]\n",
    "            \n",
    "            # Insert the data for the current date\n",
    "            \n",
    "            try:\n",
    "                #chunk_size = 500\n",
    "                #for chunk in range(0, len(group), chunk_size):\n",
    "                #    df_chunk = group.iloc[chunk:chunk + chunk_size]\n",
    "                #    df_chunk.to_sql(table_name, conn, schema=db_schema, if_exists='append', index=False)\n",
    "                #    conn.commit()\n",
    "                #    if ((chunk+chunk_size)<len(group)): prt_info(f\"Data for {date}: {chunk+chunk_size} entries inserted.\", kind=logging.DEBUG, nl=False)\n",
    "\n",
    "                # Get the raw DBAPI connection\n",
    "                raw_conn = conn.connection\n",
    "                cursor = raw_conn.cursor()\n",
    "\n",
    "                # create the buffer\n",
    "                buffer = StringIO()\n",
    "                group.to_csv(buffer, index=False, header=False) \n",
    "                buffer.seek(0) \n",
    "\n",
    "                # Use the COPY command\n",
    "                cursor.copy_expert(f\"\"\"\n",
    "                    COPY \\\"{db_schema}\\\".{table_name}({', '.join(columns)}) FROM STDIN WITH CSV DELIMITER ',';\n",
    "                \"\"\", buffer)\n",
    "                \n",
    "                prt_info(f\"Data for {date} inserted successfully ({group.shape[0]} inserted).\")\n",
    "            except Exception as e:\n",
    "                conn.rollback()\n",
    "                raise Exception(f\"An error occurred while inserting Occupancy data for {date}: {e}\")\n",
    "    \n",
    "            #register the audit table\n",
    "            try:\n",
    "                audit_query = text(f\"INSERT INTO \\\"AFC\\\".audit(timestamp, \\\"table\\\", operation, period, \\\"user\\\") VALUES (\\'{datetime.datetime.now().strftime('%Y-%m-%d %H:%M')}\\', \\'{table_name}\\', \\'insert\\', \\'{date}\\', \\'{os.getlogin()}\\')\")\n",
    "                conn.execute(audit_query)\n",
    "                conn.commit()\n",
    "            except Exception as e:\n",
    "                conn.rollback()\n",
    "                raise Exception(f\"An error occurred while writing the audit information for Occupancy: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "0af4981b-ca5f-48e5-8827-0623c4593084",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_constraints(db_schema, table_name):\n",
    "    # remove the constraints of the table\n",
    "    with alchemyEngine.connect().execution_options(autocommit=True) as conn:  \n",
    "        try:\n",
    "            constraints_query = text(f\"SELECT \\\"{db_schema}\\\".{remove_constraints_function} ('{db_schema}\\', \\'{table_name}\\')\")\n",
    "            conn.execute(constraints_query)\n",
    "            conn.commit()\n",
    "            prt_info(f\"Constraints removed from {table_name}.\")\n",
    "        except Exception as e:\n",
    "            conn.rollback()\n",
    "            prt_info(f\"An error occurred while removing the constraints of the table {table_name}: {e}\", kind=logging.ERROR)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "94238883-ac45-43d2-85c7-80663bce07f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_constraints(db_schema, add_constraints_function):\n",
    "    # add the constraints again\n",
    "    with alchemyEngine.connect().execution_options(autocommit=True) as conn:      \n",
    "        try:\n",
    "            prt_info(f\"Adding constraints...\")\n",
    "            constraints_query = text(f\"SELECT \\\"{db_schema}\\\".{add_constraints_function} ()\")\n",
    "            conn.execute(constraints_query)\n",
    "            conn.commit()\n",
    "            prt_info(f\"Constraints added.\")\n",
    "        except Exception as e:\n",
    "            conn.rollback()\n",
    "            prt_info(f\"An error occurred while adding the constraints {add_constraints_function}: {e}\", kind=logging.ERROR)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "2608d600-2a66-4726-9dc0-6865bbf26333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generic function to handle each file (calling the corresponding function)\n",
    "# this is the function to be parallelized during reading process\n",
    "def read_file(report, file, sheet_name): \n",
    "    try:\n",
    "        # reading sheet\n",
    "        is_read = True\n",
    "        if(report == 'Train List'): df_file = read_train_list(file, sheet=sheet_name)\n",
    "        elif(report == 'Booking Payment Detailed'): df_file = read_booking_payment(file, sheet=sheet_name)\n",
    "        elif(report == 'Occupancy'): df_file = read_occupancy(file, sheet=sheet_name)            \n",
    "        else:\n",
    "            prt_info(f\"Reading of files {report} have not been implemented yet.\", logging.WARNING)\n",
    "            return pd.DataFrame() #return empty dataframe\n",
    "    \n",
    "        prt_info(f\"Sheet '{sheet_name}' from '{file}' read.\")\n",
    "        return df_file\n",
    "        \n",
    "    except Exception as e:\n",
    "        prt_info(f\"Error reading the sheet {sheet_name} from file {file}:{e}.\", logging.ERROR)\n",
    "        return pd.DataFrame() #return empty dataframe\n",
    "            \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "f3216f5e-a3f2-41b5-a793-1b6ae279a5fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-03-09_12-38-25] \n",
      "**************** Exporter version 0.82 ***********************\n",
      "[2025-03-09_12-38-25] Date 2025-03-09 12:38:25.827300\n",
      "[2025-03-09_12-38-25] Found sheet 'P09_TrainList' from 'P09_TL_2025-04-10_to_2025-04-30_@_2025_03_09.nopag.xlsx' as Train List\n",
      "[2025-03-09_12-38-25] Reading Train List...\n",
      "[2025-03-09_12-38-26] Reading sheet 'P09_TrainList' from 'P09_TL_2025-04-10_to_2025-04-30_@_2025_03_09.nopag.xlsx'...\n",
      "[2025-03-09_12-38-39] 1 registers from Train List have been removed because they were incorrect entries.\n",
      "[2025-03-09_12-38-42] Sheet 'P09_TrainList' from 'P09_TL_2025-04-10_to_2025-04-30_@_2025_03_09.nopag.xlsx' read.\n",
      "[2025-03-09_12-38-43] Exporting Train List...\n",
      "[2025-03-09_12-38-43] Constraints removed from train_list.\n",
      "[2025-03-09_12-39-05] Previous data from 2025-04-10 to 2025-04-30 deleted successfully.\n",
      "[2025-03-09_12-39-06] Data for 2025-04-10 inserted successfully (1431 inserted).\n",
      "[2025-03-09_12-39-06] Data for 2025-04-11 inserted successfully (1734 inserted).\n",
      "[2025-03-09_12-39-07] Data for 2025-04-12 inserted successfully (1664 inserted).\n",
      "[2025-03-09_12-39-08] Data for 2025-04-13 inserted successfully (1518 inserted).\n",
      "[2025-03-09_12-39-08] Data for 2025-04-14 inserted successfully (1843 inserted).\n",
      "[2025-03-09_12-39-09] Data for 2025-04-15 inserted successfully (1405 inserted).\n",
      "[2025-03-09_12-39-09] Data for 2025-04-16 inserted successfully (1177 inserted).\n",
      "[2025-03-09_12-39-10] Data for 2025-04-17 inserted successfully (976 inserted).\n",
      "[2025-03-09_12-39-10] Data for 2025-04-18 inserted successfully (385 inserted).\n",
      "[2025-03-09_12-39-10] Data for 2025-04-19 inserted successfully (596 inserted).\n",
      "[2025-03-09_12-39-11] Data for 2025-04-20 inserted successfully (494 inserted).\n",
      "[2025-03-09_12-39-11] Data for 2025-04-21 inserted successfully (482 inserted).\n",
      "[2025-03-09_12-39-11] Data for 2025-04-22 inserted successfully (320 inserted).\n",
      "[2025-03-09_12-39-11] Data for 2025-04-23 inserted successfully (339 inserted).\n",
      "[2025-03-09_12-39-11] Data for 2025-04-24 inserted successfully (272 inserted).\n",
      "[2025-03-09_12-39-11] Data for 2025-04-25 inserted successfully (178 inserted).\n",
      "[2025-03-09_12-39-12] Data for 2025-04-26 inserted successfully (182 inserted).\n",
      "[2025-03-09_12-39-12] Data for 2025-04-27 inserted successfully (128 inserted).\n",
      "[2025-03-09_12-39-12] Data for 2025-04-28 inserted successfully (159 inserted).\n",
      "[2025-03-09_12-39-12] Data for 2025-04-29 inserted successfully (123 inserted).\n",
      "[2025-03-09_12-39-12] Data for 2025-04-30 inserted successfully (79 inserted).\n",
      "[2025-03-09_12-39-12] Adding constraints...\n",
      "[2025-03-09_12-52-50] Constraints added.\n",
      "[2025-03-09_12-52-50] Report Train List exported successfully.\n",
      "[2025-03-09_12-52-50] Exportation finished.\n"
     ]
    }
   ],
   "source": [
    "# ************************************* MAIN PROGRAM *****************************************************\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "#debug\n",
    "tiers = []\n",
    "dfs = []\n",
    "\n",
    "db_schema = \"AFC\"\n",
    "tbl_train_list = \"train_list\"\n",
    "tbl_bpd = \"booking_payment_detailed\"\n",
    "\n",
    "remove_constraints_function = 'drop_all_constraints'\n",
    "add_bpd_constraints_function = 'recreate_bpd_constraints'\n",
    "add_constraints_function = 'recreate_tl_constraints'\n",
    "\n",
    "prt_info(f\"\\n**************** Exporter version {version} ***********************\")\n",
    "prt_info(f\"Date {datetime.datetime.today()}\")\n",
    "files_found = {}\n",
    "\n",
    "# get all xlsx files\n",
    "for file in os.listdir(\".\"):\n",
    "    if file.endswith(\".xlsx\"):\n",
    "        # check all the sheets on the report\n",
    "        excel_file = pd.ExcelFile(file)\n",
    "\n",
    "        # loop over the sheets\n",
    "        for sheet_name in excel_file.sheet_names:\n",
    "            # check if it is a report\n",
    "            first_row, kind_file = get_report_name(file, sheet_name)\n",
    "            \n",
    "            if(kind_file != NO_REPORT):\n",
    "                # get the name\n",
    "                if(kind_file == TRAIN_LIST_REPORT): kind_file_name = 'Train List'\n",
    "                elif(kind_file == BOOKING_PAYMENT_REPORT): kind_file_name = 'Booking Payment Detailed'\n",
    "                elif(kind_file == OCCUPANCY_REPORT): kind_file_name = 'Occupancy'\n",
    "                else: kind_file_name = 'Unknown'\n",
    "                prt_info(f\"Found sheet '{sheet_name}' from '{file}' as {kind_file_name}\")\n",
    "    \n",
    "                # add the file and the kind to the list\n",
    "                if not kind_file_name in files_found:\n",
    "                    files_found[kind_file_name] = []\n",
    "                    \n",
    "                if [file, sheet_name] not in files_found[kind_file_name]:\n",
    "                    files_found[kind_file_name].append([kind_file_name, file, sheet_name])\n",
    "            \n",
    "            else: prt_info(f\"No report found in sheet '{sheet_name}' from '{file}'\", kind=logging.WARNING)\n",
    "\n",
    "        # Close the Excel file\n",
    "        excel_file.close()\n",
    "\n",
    "# if there are no files, exit\n",
    "if(len(files_found) == 0):\n",
    "    prt_info(\"No valid files found. Exiting program\", logging.WARNING)\n",
    "    sys.exit()\n",
    "\n",
    "# export each report one by one\n",
    "for report in files_found:\n",
    "    df = pd.DataFrame()\n",
    "    prt_info(f\"Reading {report}...\")\n",
    "    \n",
    "    # check all the files associated\n",
    "    for report_name, file, sheet_name in files_found[report][:]:\n",
    "        # reading sheet\n",
    "        try:\n",
    "            df = pd.concat([df, read_file(report_name, file, sheet_name)])    \n",
    "        except Exception as e:\n",
    "            prt_info(f\"Error on line {get_error_line(e)}: Reading of sheet '{sheet_name}' from '{file}' failed: {e}\", logging.ERROR)\n",
    "            files_found[report].remove([report_name, file, sheet_name])\n",
    "\n",
    "    # if there is data\n",
    "    if df.empty:\n",
    "        prt_info(f'No data to export for report {report}', logging.WARNING)\n",
    "    else:\n",
    "        # order the dataframe\n",
    "        if(report == 'Train List'): sort_by = ['departure_date', 'operation_date_time']\n",
    "        elif(report == 'Booking Payment Detailed'): sort_by = ['operation_date_time']\n",
    "        elif(report == 'Occupancy'): sort_by = ['ticket_reserved', 'quota_configuration']\n",
    "            \n",
    "        df.sort_values(by= sort_by, ascending=True, inplace=True)\n",
    "    \n",
    "        # remove duplicates\n",
    "        if(report == 'Train List'): subset_col = ['ticket_number']\n",
    "        elif(report == 'Booking Payment Detailed'): subset_col = None\n",
    "        elif(report == 'Occupancy'): subset_col = ['date', 'od','train_number', 'class']\n",
    "    \n",
    "        if subset_col is not None:\n",
    "            duplicates = df.duplicated(subset=subset_col, keep='last')\n",
    "            if(duplicates.sum() > 0):\n",
    "                prt_info(f\"Deleting {duplicates.sum()} duplicated entries.\")\n",
    "                df[duplicates].to_csv(f\"{export_folder}/{report} duplicates {current_time}.csv.zip\")\n",
    "                df.drop_duplicates(subset=subset_col, keep='last', inplace=True, ignore_index=True)\n",
    "                os.makedirs(export_folder, exist_ok=True)      \n",
    "                    \n",
    "        # save the results\n",
    "        # Check if the directory exists, and if not, create it\n",
    "        if not os.path.exists(export_folder):\n",
    "            os.makedirs(export_folder)\n",
    "        df.to_csv(f\"{export_folder}/{report} data exported {current_time}.csv.zip\", index=False, encoding='utf-8')\n",
    "        \n",
    "        # export the valid files\n",
    "        prt_info(f\"Exporting {report}...\")\n",
    "        is_above_threshold = df.shape[0] > rows_threshold_constraint_removal\n",
    "        #debug\n",
    "        #is_above_threshold = True\n",
    "        try:\n",
    "            if(report == 'Train List'):\n",
    "                if is_above_threshold: remove_constraints(db_schema, tbl_train_list)\n",
    "                export_train_list(df, db_schema)\n",
    "                if is_above_threshold: add_constraints('AFC','recreate_tl_constraints')\n",
    "                prt_info(f\"Report {report} exported successfully.\")\n",
    "            elif(report == 'Booking Payment Detailed'):\n",
    "                if is_above_threshold: remove_constraints(db_schema, tbl_bpd)\n",
    "                export_booking_payment(df, db_schema)\n",
    "                if is_above_threshold: add_constraints('AFC','recreate_bpd_constraints')\n",
    "                prt_info(f\"Report {report} exported successfully.\")\n",
    "            elif(report == 'Occupancy'):\n",
    "                export_occupancy(df, db_schema)\n",
    "                prt_info(f\"Report {report} exported successfully.\")\n",
    "            else:\n",
    "                prt_info(f\"Exportation of report {report} have not been implemented yet.\", kind=logging.WARNING)\n",
    "        except Exception as e:\n",
    "            prt_info(f\"Error on line {get_error_line(e)} while exporting {report}: {e}\", kind=logging.ERROR)\n",
    "            prt_info(f\"Exportation of {report} failed. Exportation of the report aborted.\", kind=logging.ERROR)\n",
    "\n",
    "\n",
    "\n",
    "# move the original files to a subdirectory to arrange the output\n",
    "if len(files_found) > 0:\n",
    "    if not os.path.exists(data_folder):\n",
    "        os.makedirs(data_folder)\n",
    "\n",
    "    for report in files_found:\n",
    "        for report_name, file, sheet in files_found[report]:\n",
    "            if os.path.exists(file):\n",
    "                if os.path.exists(f\"{data_folder}/{file}\"):\n",
    "                    os.remove(f\"{data_folder}/{file}\")  # Remove the file if it already exists\n",
    "                shutil.move(file, data_folder)\n",
    "\n",
    "\n",
    "# finish\n",
    "prt_info(\"Exportation finished.\")\n",
    "\n",
    "error_file = error_handler.baseFilename\n",
    "logging.shutdown()\n",
    "\n",
    "# if there is an error, generate an alert window\n",
    "if(errors_found):\n",
    "    messagebox.showinfo(\"Alert\", \"There were ERRORS or WARNINGS during the exportation process. Please check file error_\" + log_name + \" for details.\")\n",
    "else:\n",
    "    # delete the error log\n",
    "    if os.path.exists(error_file):\n",
    "        if os.stat(error_file).st_size == 0:\n",
    "            os.remove(error_file)\n",
    "\n",
    "    messagebox.showinfo(\"Alert\", \"The exportation process has been completed SUCCESSFUL.\")\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
